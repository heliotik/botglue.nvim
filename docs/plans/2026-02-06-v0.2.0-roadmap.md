# botglue.nvim v0.2.0 Roadmap

## Vision

Transition from "4 hardcoded operations with notification popups" to "unified smart editing agent with inline feedback, full-file context, and Claude Code skills integration." The user stays in their flow, the AI works as an inline assistant — not a separate modal experience.

## Priority Matrix

| Priority | Feature                                               | Impact       | Effort |
| -------- | ----------------------------------------------------- | ------------ | ------ |
| P0       | Inline progress display (extmarks)                    | Critical UX  | Medium |
| P0       | Request cancellation                                  | Critical UX  | Small  |
| P0       | Unify operations into single smart command            | Architecture | Large  |
| P1       | Full-file context + selection awareness               | Quality      | Medium |
| P1       | Model selection                                       | Flexibility  | Small  |
| P1       | Better master prompt + skills integration             | Quality      | Large  |
| P2       | Prompt history with fzf search                        | UX           | Medium |
| P2       | Debug logging                                         | DX           | Medium |
| P3       | UI cleanup (remove Russian labels, title → "botglue") | Polish       | Small  |
| P3       | Streaming output from Claude Code                     | UX           | Medium |

---

## P0: Critical — Must Ship

### 1. Inline Progress Display (extmarks)

**Problem:** `vim.notify` in the corner gives no understanding of what's happening. User loses context of where the operation is running.

**Solution:** Replace spinner + notification with inline virtual text (extmarks), similar to how 99 plugin works:

- Place a `virt_lines` extmark **below** the last line of selection: spinner + short description of what's happening
- Place a `virt_lines` extmark **above** the first line of selection: the prompt text (truncated) so user remembers what they asked
- User can continue editing other parts of the file while waiting
- On completion: extmarks are removed, selection is replaced (or result window opens)

**Implementation:**

```
-- New module: lua/botglue/display.lua
-- Uses nvim_buf_set_extmark with virt_lines option
-- Namespace: vim.api.nvim_create_namespace("botglue")
--
-- display.show_progress(bufnr, start_line, end_line, prompt_text)
-- display.update_progress(bufnr, spinner_frame, status_text)
-- display.clear(bufnr)
```

**Key API:** `nvim_buf_set_extmark()` with `virt_lines` and `virt_lines_above` options. This renders virtual lines in the buffer that don't affect actual content.

**Reference:** Study `ThePrimeagen/99` — `lua/99/display/` module for how they render inline status.

### 2. Request Cancellation

**Problem:** No way to cancel a long-running or incorrect request. Everything freezes until Claude responds.

**Solution:**

- Store `job_id` from `vim.fn.jobstart()` in module state
- Add `M.cancel()` function that calls `vim.fn.jobstop(job_id)`
- Keymap: `<leader>ps` (stop) or `<Esc>` while progress is shown
- On cancel: clear extmarks, notify "Cancelled", restore original text if needed
- Add configurable timeout (default: 60s) with auto-cancel

**Implementation:**

```lua
-- In claude.lua: track active job
M._active_job = nil

function M.cancel()
  if M._active_job and M._active_job > 0 then
    vim.fn.jobstop(M._active_job)
    M._active_job = nil
  end
end
```

### 3. Unify Operations into Single Smart Command

**Problem:** 4 separate commands (`optimize`, `explain`, `refactor`, `translate`) are rigid. The user should describe what they want, not pick from a menu.

**Solution:** Single entry point `:Botglue` (or `<leader>pp`) that:

1. Takes the visual selection
2. Opens input window where user types their intent (e.g., "simplify this", "translate to English", "add error handling")
3. Claude Code determines the appropriate action from context:
   - Selection content + full file context + user prompt → Claude decides
   - Skills system helps route to the right behavior

**Keep legacy commands** as aliases/shortcuts for common patterns, but the primary flow is unified.

**New architecture:**

```
:Botglue              → unified smart command (primary)
:BotglueOptimize      → shortcut: pre-fills prompt with optimize intent
:BotglueExplain       → shortcut: pre-fills prompt with explain intent
:BotglueRefactor      → shortcut: pre-fills prompt with refactor intent
:BotglueTranslate     → shortcut: pre-fills prompt with translate intent
```

**This requires rethinking `claude.lua`:**

- Instead of `PROMPTS` table with hardcoded templates, build a smart system prompt via `--append-system-prompt`
- Use Claude Code's native understanding + project context (CLAUDE.md, `.claude/skills/`)
- Pass full file as context, highlight selected region

---

## P1: Important — Should Ship

### 4. Full-File Context + Selection Awareness

**Problem:** Currently only the selected text is sent. Claude has no idea what surrounds it — function signatures, imports, module structure.

**Solution:** Send the entire file content with the selected region marked:

```
File: lua/botglue/config.lua (lua)
Project: botglue.nvim

Full file content:
---
local M = {}

M.defaults = {
  model = "opus",
  default_keymaps = true,
}

--- SELECTED REGION START (lines 8-12) ---
M.options = {}

function M.setup(opts)
  M.options = vim.tbl_deep_extend("force", {}, M.defaults, opts or {})
end
--- SELECTED REGION END ---

return M
---

User request: <user's prompt here>
```

**Considerations:**

- For large files, truncate to ~500 lines around selection (configurable)
- Use `--append-system-prompt` to set behavior rules
- The system prompt instructs Claude to modify only the selected region unless the user explicitly asks for broader changes

### 5. Model Selection

**Problem:** All requests go through `opus` which is slow for simple tasks. No way to pick a faster model.

**Solution:**

```lua
require("botglue").setup({
  model = "opus",              -- default model
  models = {                   -- available models for quick switching
    { name = "opus", alias = "o" },
    { name = "sonnet", alias = "s" },
    { name = "haiku", alias = "h" },
  },
})
```

**UX options:**

- **Prefix in input window:** Type `!h simplify this` to use haiku, `!s` for sonnet, default = config model
- **Separate keymap:** `<leader>ph` for haiku-mode quick edit
- **Config-level:** `model = "sonnet"` to change default globally

**Implementation:** Pass `--model` flag to `claude` CLI based on selection.

### 6. Better Master Prompt + Skills Integration

**Problem:** Current prompts are simplistic templates. They don't leverage Claude Code's capabilities (tool use, file reading, project understanding).

**Solution:** Two-layer approach:

**Layer 1: Smart system prompt** (`--append-system-prompt`)

Build a dynamic system prompt that includes:

- Project name, file path, filetype
- Whether result should replace selection or show in window
- Operating mode (edit code / explain / translate / free-form)
- Reference to project's CLAUDE.md if it exists

**Layer 2: Claude Code skills** (`.claude/skills/botglue/`)

Create bundled skills that ship with the plugin and get copied to the project:

```
.claude/skills/botglue-edit/
├── SKILL.md          # Instructions for code editing operations
└── examples/
    └── refactor.md   # Example of good refactoring output

.claude/skills/botglue-explain/
├── SKILL.md          # Instructions for explanation operations
```

**Research needed:**

- Read https://code.claude.com/docs/en/skills — understand how `-p` mode interacts with skills
- Study `--append-system-prompt` vs `--system-prompt` tradeoffs
- Test whether Claude Code in `-p` mode auto-discovers `.claude/skills/`
- Investigate `--allowedTools` to restrict tool use for simple edits (speed improvement)

---

## P2: Nice to Have — Ship If Time Allows

### 7. Prompt History with fzf Search

**Problem:** Users retype similar prompts. No memory of previous requests.

**Solution:**

- Store prompt history in `~/.local/share/nvim/botglue/history.json`
- Format: `{ prompt: string, count: number, last_used: timestamp, model: string }`
- Sort by frequency (most used first)
- Search via telescope or fzf-lua picker

**UX flow:**

1. User presses `<leader>pp` → input window opens
2. If input is empty and user presses `<C-p>`, telescope/fzf picker opens with history
3. Select from history → populates input → user can edit and submit
4. Alternatively: `<leader>pP` opens picker directly

**Implementation:**

```lua
-- New module: lua/botglue/history.lua
M.add(prompt, model)
M.get_sorted()           -- returns list sorted by frequency
M.search(query)           -- fuzzy search
M.save() / M.load()
```

### 8. Debug Logging

**Problem:** When something goes wrong, there's no way to understand what happened. "При рефакторинге всё повисает" — no diagnostics.

**Solution:**

- Structured logging module (`lua/botglue/log.lua`)
- Log levels: DEBUG, INFO, WARN, ERROR
- Log to file: `~/.local/share/nvim/botglue/debug.log`
- Log what matters:
  - Full prompt sent to Claude
  - CLI command with all flags
  - stdout/stderr streams
  - Timing (request start → first byte → complete)
  - Exit code

**Config:**

```lua
require("botglue").setup({
  log = {
    level = "WARN",          -- DEBUG | INFO | WARN | ERROR
    path = nil,              -- auto: ~/.local/share/nvim/botglue/debug.log
    print_on_error = true,   -- vim.notify on errors
  },
})
```

**Commands:**

- `:BotglueLog` — open log file in split
- `:BotglueLogClear` — clear log

---

## P3: Polish

### 9. UI Cleanup

- Replace all Russian UI strings with English or neutral labels
- Window title: `" botglue "` (not operation-specific)
- Input window prompt: just `" botglue "` with placeholder hint
- Configurable locale later if needed

### 10. Streaming Output (Exploratory)

**Problem:** Long responses block until complete. User sees nothing during processing.

**Solution:** Use `--output-format stream-json` with Claude CLI to get incremental output.

- Parse newline-delimited JSON from stdout
- For replace mode: show diff preview as it streams
- For explain mode: render text into result window incrementally

**This is exploratory** — needs investigation of how `stream-json` works with `vim.fn.jobstart` buffered vs unbuffered stdout.

---

## Technical Research Tasks

Before implementation, study these resources:

| Resource                                                                        | What to Learn                                                                             |
| ------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| [Claude Code CLI Reference](https://code.claude.com/docs/en/cli-reference.md)   | All available flags, especially `--append-system-prompt`, `--allowedTools`, `--max-turns` |
| [Claude Code Output Styles](https://code.claude.com/docs/en/output-styles.md)   | `stream-json` format for streaming                                                        |
| [Claude Code Headless Mode](https://code.claude.com/docs/en/headless.md)        | Session management (`--continue`, `--resume`), input-format for multi-turn                |
| [Claude Code Best Practices](https://code.claude.com/docs/en/best-practices.md) | Context management, subagents, scratchpads                                                |
| [Claude Code Skills](https://code.claude.com/docs/en/skills.md)                 | SKILL.md format, auto-discovery, how skills work in `-p` mode                             |
| [ThePrimeagen/99 source](https://github.com/ThePrimeagen/99)                    | Display module (extmarks), request lifecycle, AGENT.md integration, stop_all_requests     |
| [ThePrimeagen 99 origin video](https://youtube.com/watch?v=MNvciiZ5x9A)         | Design philosophy, what problems it solves                                                |
| [ThePrimeagen 99 demo](https://youtube.com/watch?v=ws9zR-UzwTE)                 | UX patterns, how inline feedback works                                                    |

---

## Implementation Order

```
Phase 1: Foundation (P0)
  ├── 1.1 display.lua — extmark-based inline progress
  ├── 1.2 Request cancellation + timeout
  └── 1.3 Unified command architecture

Phase 2: Context & Quality (P1)
  ├── 2.1 Full-file context with selection markers
  ├── 2.2 Model selection (prefix + config)
  └── 2.3 Smart system prompt (dynamic, project-aware)

Phase 3: DX & History (P2)
  ├── 3.1 Debug logging module
  └── 3.2 Prompt history + fzf picker

Phase 4: Polish (P3)
  ├── 4.1 UI string cleanup
  └── 4.2 Streaming output (if feasible)
```

Each phase should end with `make pr-ready` passing and a tagged pre-release.

---

## Open Questions

1. **Does Claude Code `-p` mode auto-discover `.claude/skills/`?** If yes, we can ship bundled skills. If not, we need `--append-system-prompt` for everything.

2. **`--allowedTools` for speed:** Can we restrict Claude to `Read` only (no `Bash`, no `Write`) for simple text edits? This could dramatically speed up responses by preventing tool-use loops.

3. **Session continuity:** Should botglue support `--continue` / `--resume` for iterative edits on the same selection? E.g., "now also add type annotations" continuing from last edit.

4. **Telescope vs fzf-lua dependency:** For prompt history picker — make it optional with fallback to `vim.ui.select`? Or require one of them?

5. **Streaming reliability:** How does `stream-json` interact with `vim.fn.jobstart`'s `stdout_buffered`? Need to test with `stdout_buffered = false` and manual buffer assembly.
